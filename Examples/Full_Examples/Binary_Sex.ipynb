{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a simple binary classification example, explaining general library functionality along the way.\n",
    "Within this notebook we make use of data downloaded from Release 2.0.1 of the the ABCD Study (https://abcdstudy.org/). This dataset is openly avaliable to researchers (after signing a data use agreement) and is particullarly well suited towards performing neuroimaging based ML given the large sample size of the study.\n",
    "\n",
    "Within this notebook we will be performing binary classification predicting sex assigned at birth from tabular ROI structural MRI data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the main BPt object\n",
    "from BPt import BPt_ML\n",
    "\n",
    "# The rest of these are just used to help show different things\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following definition are useful simply as a shorthand for defining long file paths all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base data directories, 2.0 release with most of the phenotype information\n",
    "nda_dr = '/home/sage/work/ABCD2p0NDA/'\n",
    "\n",
    "# This folder contains the re-released 2.0.1 fixed MRI derived measurements\n",
    "nda_dr2 = '/home/sage/work/ABCDFixRelease2p0p1/'\n",
    "\n",
    "#This file stores the name mapping\n",
    "map_file = os.path.join(nda_dr2, 'Fix Release Notes 2.0.1_Public', '24. ABCD_Release_2.0.1_Updates',\n",
    "                        'abcd_2.0.1_mapping.csv')\n",
    "\n",
    "# Destr atlas structural MRI rois\n",
    "data1 = os.path.join(nda_dr2, 'abcd_mrisdp101.txt')\n",
    "data2 = os.path.join(nda_dr2, 'abcd_mrisdp201.txt')\n",
    "\n",
    "# Family ID\n",
    "strat1 = os.path.join(nda_dr, 'acspsw03.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to define the class object, which we will use to load load and to train/test different ML models.\n",
    "There are a few global parameters which we can optionally set when defining this object as well. Within a notebook context the easiest way to view the relevent documentation is to simply type BPt_ML? within a cell, and click run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the default parameters are okay for this simple example, but any of them can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_name = Sex\n",
      "log_dr = /home/sage/BPt/Examples/Full_Examples\n",
      "existing_log = overwrite\n",
      "verbose = True\n",
      "exp log dr setup at: /home/sage/BPt/Examples/Full_Examples/Sex\n",
      "log file at: /home/sage/BPt/Examples/Full_Examples/Sex/logs.txt\n",
      "Default params set:\n",
      "notebook = True\n",
      "use_abcd_subject_ids = True\n",
      "low memory mode = False\n",
      "strat_u_name = _Strat\n",
      "random state = 1\n",
      "n_jobs = 1\n",
      "dpi = 100\n",
      "mp_context = spawn\n",
      "BPt_ML object initialized\n"
     ]
    }
   ],
   "source": [
    "ML = BPt_ML(exp_name = 'Sex',\n",
    "             existing_log = 'overwrite',\n",
    "             use_abcd_subject_ids = True,\n",
    "             random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will consider setting default loading parameters. Argument documentation can be viewed in the same way as discussed before, via ML.Set_Default_Load_Params?, or by visting BPt's documentation at https://bpt.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default load params set within self.default_load_params.\n",
      "----------------------\n",
      "dataset_type: basic\n",
      "subject_id: src_subject_id\n",
      "eventname: baseline_year_1_arm_1\n",
      "eventname_col: eventname\n",
      "overlap_subjects: False\n",
      "merge: inner\n",
      "na_values: ['777', '999']\n",
      "drop_na: True\n",
      "drop_or_na: drop\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Set_Default_Load_Params(dataset_type = 'basic',\n",
    "                           eventname = 'baseline_year_1_arm_1',\n",
    "                           eventname_col = 'eventname',\n",
    "                           drop_na = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue by optionally loading in a name map, which is simply a dictionary that attempts to rename any column names loaded in, if those column names are a key in the dictionary. This is useful for ABCD data as the default column names might not be useful. This mapping was provided along with the 2.0.1 data release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/sage/work/ABCDFixRelease2p0p1/Fix Release Notes 2.0.1_Public/24. ABCD_Release_2.0.1_Updates/abcd_2.0.1_mapping.csv  with dataset type: explorer\n",
      "Loading new name_map from file!\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Name_Map(loc = map_file,\n",
    "                 dataset_type = 'explorer',\n",
    "                 source_name_col = 'nda_name',\n",
    "                 target_name_col = 'deap_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ddtidp_119': 'dmri_dti.fa.wm_cort.destrieux_s.central.rh',\n",
       " 'ddtidp_120': 'dmri_dti.fa.wm_cort.destrieux_s.cingul.marginalis.rh',\n",
       " 'ddtidp_121': 'dmri_dti.fa.wm_cort.destrieux_s.circular.insula.ant.rh',\n",
       " 'ddtidp_122': 'dmri_dti.fa.wm_cort.destrieux_s.circular.insula.inf.rh',\n",
       " 'ddtidp_123': 'dmri_dti.fa.wm_cort.destrieux_s.circular.insula.sup.rh',\n",
       " 'ddtidp_124': 'dmri_dti.fa.wm_cort.destrieux_s.collat.transv.ant.rh',\n",
       " 'ddtidp_125': 'dmri_dti.fa.wm_cort.destrieux_s.collat.transv.post.rh',\n",
       " 'ddtidp_126': 'dmri_dti.fa.wm_cort.destrieux_s.front.inf.rh',\n",
       " 'ddtidp_127': 'dmri_dti.fa.wm_cort.destrieux_s.front.middle.rh',\n",
       " 'ddtidp_128': 'dmri_dti.fa.wm_cort.destrieux_s.front.sup.rh',\n",
       " 'ddtidp_129': 'dmri_dti.fa.wm_cort.destrieux_s.interm.prim.jensen.rh',\n",
       " 'ddtidp_130': 'dmri_dti.fa.wm_cort.destrieux_s.intrapariet.and.p.trans.rh',\n",
       " 'ddtidp_131': 'dmri_dti.fa.wm_cort.destrieux_s.oc.middle.and.lunatus.rh',\n",
       " 'ddtidp_132': 'dmri_dti.fa.wm_cort.destrieux_s.oc.sup.and.transversal.rh',\n",
       " 'ddtidp_133': 'dmri_dti.fa.wm_cort.destrieux_s.occipital.ant.rh',\n",
       " 'ddtidp_134': 'dmri_dti.fa.wm_cort.destrieux_s.oc.temp.lat.rh',\n",
       " 'ddtidp_135': 'dmri_dti.fa.wm_cort.destrieux_s.oc.temp.med.and.lingual.rh',\n",
       " 'ddtidp_136': 'dmri_dti.fa.wm_cort.destrieux_s.orbital.lateral.rh',\n",
       " 'ddtidp_137': 'dmri_dti.fa.wm_cort.destrieux_s.orbital.med.olfact.rh',\n",
       " 'ddtidp_138': 'dmri_dti.fa.wm_cort.destrieux_s.orbital.h.shaped.rh'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_examples = {k: ML.name_map[k] for k in list(ML.name_map)[300:320]}\n",
    "some_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to consider is in if there are any subjects which should be apriori excluded. These can be defined as either a list of subjects to exclude, or as a list of subjects to include (where only those subjects that appear in the list would be kept).\n",
    "\n",
    "See:\n",
    "\n",
    "- https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#load-exclusions\n",
    "\n",
    "- https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#load-inclusions\n",
    "\n",
    "In this example we will not define any.\n",
    "\n",
    "Next, we will load in the actual data of interest. You will note when looking at the help function that all of the parameter we set in the defaults can also be set here as well, or if we want to keep the default params, we just leave the values as is.\n",
    "\n",
    "See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#load-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp101.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp101_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp201.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp201_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 2892 rows for missing values, based on the provided drop_na param: True with actual na_thresh: 0\n",
      "Loaded rows with NaN remaining: 0\n",
      "\n",
      "Processing unique col values with drop threshold: 0 - warn threshold: 432.1 - out of 8642 rows\n",
      "Warn - smri_area_cort.destrieux_g.front.inf.orbital.lh has unique vals: 300\n",
      "Warn - smri_area_cort.destrieux_g.ins.lg.and.s.cent.ins.lh has unique vals: 350\n",
      "Warn - smri_area_cort.destrieux_g.temp.sup.g.t.transv.lh has unique vals: 417\n",
      "Warn - smri_area_cort.destrieux_lat.fis.ant.horizont.lh has unique vals: 300\n",
      "Warn - smri_area_cort.destrieux_lat.fis.ant.vertical.lh has unique vals: 395\n",
      "Warn - smri_area_cort.destrieux_s.circular.insula.ant.lh has unique vals: 398\n",
      "Warn - smri_area_cort.destrieux_s.collat.transv.post.lh has unique vals: 427\n",
      "Warn - smri_area_cort.destrieux_s.orbital.lateral.lh has unique vals: 407\n",
      "Warn - smri_area_cort.destrieux_s.temporal.transverse.lh has unique vals: 371\n",
      "Warn - smri_area_cort.destrieux_g.cingul.post.ventral.rh has unique vals: 302\n",
      "Warn - smri_area_cort.destrieux_g.front.inf.orbital.rh has unique vals: 351\n",
      "Warn - smri_area_cort.destrieux_g.ins.lg.and.s.cent.ins.rh has unique vals: 355\n",
      "Warn - smri_area_cort.destrieux_g.rectus.rh has unique vals: 399\n",
      "Warn - smri_area_cort.destrieux_g.temp.sup.g.t.transv.rh has unique vals: 321\n",
      "Warn - smri_area_cort.destrieux_lat.fis.ant.horizont.rh has unique vals: 361\n",
      "Warn - smri_area_cort.destrieux_lat.fis.ant.vertical.rh has unique vals: 328\n",
      "Warn - smri_area_cort.destrieux_s.temporal.transverse.rh has unique vals: 290\n",
      "\n",
      "Loaded Shape: (8642, 1510)\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Data(loc=[data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.frontomargin.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.occipital.inf.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.paracentral.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.subcentral.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.transv.frontopol.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.cingul.ant.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.cingul.mid.ant.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.and.s.cingul.mid.post.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.cingul.post.dorsal.lh</th>\n",
       "      <th>smri_thick_cort.destrieux_g.cingul.post.ventral.lh</th>\n",
       "      <th>...</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.precentral.inf.part.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.precentral.sup.part.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.suborbital.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.subparietal.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.temporal.inf.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.temporal.sup.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_s.temporal.transverse.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_mean.lh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_mean.rh</th>\n",
       "      <th>smri_t2w.contrast_cort.destrieux_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NDAR_INVTK4DJ93H</th>\n",
       "      <td>2.566</td>\n",
       "      <td>2.264</td>\n",
       "      <td>2.129</td>\n",
       "      <td>2.991</td>\n",
       "      <td>3.079</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.757</td>\n",
       "      <td>2.758</td>\n",
       "      <td>3.207</td>\n",
       "      <td>3.04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064891</td>\n",
       "      <td>-0.054464</td>\n",
       "      <td>-0.074359</td>\n",
       "      <td>-0.070335</td>\n",
       "      <td>-0.076025</td>\n",
       "      <td>-0.071395</td>\n",
       "      <td>-0.079813</td>\n",
       "      <td>-0.048968</td>\n",
       "      <td>-0.047173</td>\n",
       "      <td>-0.048065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1510 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  smri_thick_cort.destrieux_g.and.s.frontomargin.lh  \\\n",
       "src_subject_id                                                        \n",
       "NDAR_INVTK4DJ93H                                              2.566   \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.occipital.inf.lh  \\\n",
       "src_subject_id                                                         \n",
       "NDAR_INVTK4DJ93H                                              2.264    \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.paracentral.lh  \\\n",
       "src_subject_id                                                       \n",
       "NDAR_INVTK4DJ93H                                             2.129   \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.subcentral.lh  \\\n",
       "src_subject_id                                                      \n",
       "NDAR_INVTK4DJ93H                                            2.991   \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.transv.frontopol.lh  \\\n",
       "src_subject_id                                                            \n",
       "NDAR_INVTK4DJ93H                                              3.079       \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.cingul.ant.lh  \\\n",
       "src_subject_id                                                      \n",
       "NDAR_INVTK4DJ93H                                             2.93   \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.cingul.mid.ant.lh  \\\n",
       "src_subject_id                                                          \n",
       "NDAR_INVTK4DJ93H                                              2.757     \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.and.s.cingul.mid.post.lh  \\\n",
       "src_subject_id                                                           \n",
       "NDAR_INVTK4DJ93H                                              2.758      \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.cingul.post.dorsal.lh  \\\n",
       "src_subject_id                                                        \n",
       "NDAR_INVTK4DJ93H                                              3.207   \n",
       "\n",
       "                  smri_thick_cort.destrieux_g.cingul.post.ventral.lh  ...  \\\n",
       "src_subject_id                                                        ...   \n",
       "NDAR_INVTK4DJ93H                                               3.04   ...   \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.precentral.inf.part.rh  \\\n",
       "src_subject_id                                                                \n",
       "NDAR_INVTK4DJ93H                                          -0.064891           \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.precentral.sup.part.rh  \\\n",
       "src_subject_id                                                                \n",
       "NDAR_INVTK4DJ93H                                          -0.054464           \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.suborbital.rh  \\\n",
       "src_subject_id                                                       \n",
       "NDAR_INVTK4DJ93H                                         -0.074359   \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.subparietal.rh  \\\n",
       "src_subject_id                                                        \n",
       "NDAR_INVTK4DJ93H                                          -0.070335   \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.temporal.inf.rh  \\\n",
       "src_subject_id                                                         \n",
       "NDAR_INVTK4DJ93H                                          -0.076025    \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.temporal.sup.rh  \\\n",
       "src_subject_id                                                         \n",
       "NDAR_INVTK4DJ93H                                          -0.071395    \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_s.temporal.transverse.rh  \\\n",
       "src_subject_id                                                                \n",
       "NDAR_INVTK4DJ93H                                          -0.079813           \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_mean.lh  \\\n",
       "src_subject_id                                               \n",
       "NDAR_INVTK4DJ93H                                 -0.048968   \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_mean.rh  \\\n",
       "src_subject_id                                               \n",
       "NDAR_INVTK4DJ93H                                 -0.047173   \n",
       "\n",
       "                  smri_t2w.contrast_cort.destrieux_mean  \n",
       "src_subject_id                                           \n",
       "NDAR_INVTK4DJ93H                              -0.048065  \n",
       "\n",
       "[1 rows x 1510 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at what data is loaded via ML.data\n",
    "ML.data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the loading data function has warned us about some columns with a suspiciously low number of unique values, and also note that they are all surface area, so maybe that is expected. Lets turn warn off in the future\n",
    "\n",
    "The other alarming thing to note is we are losing 2892 rows for missing values. One way we can examine this further is to re-load the data, but without dropping any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared loaded data.\n",
      "\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp101.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp101_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp201.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp201_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "\n",
      "Dropped 0 cols for all missing values\n",
      "Loaded rows with NaN remaining: 2892\n",
      "Loaded NaN Info:\n",
      "There are: 284244 total missing values\n",
      "437 columns found with 4 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "435 columns found with 617 missing values (column name overlap: ['_cort.destrieux_', 'smri_t2w.'])\n",
      "18 columns found with 1 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "12 columns found with 5 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "11 columns found with 8 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "10 columns found with 2 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "10 columns found with 3 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "7 columns found with 9 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "6 columns found with 12 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "6 columns found with 621 missing values (column name overlap: ['_cort.destrieux_', 'smri_t2w.', '.rh'])\n",
      "5 columns found with 35 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "5 columns found with 43 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "4 columns found with 39 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 's.'])\n",
      "3 columns found with 7 missing values (column name overlap: ['_cort.destrieux_mean.lh', 'smri_t1w.'])\n",
      "3 columns found with 6 missing values (column name overlap: ['_cort.destrieux_s.interm.prim.jensen.lh', 'smri_t1w.'])\n",
      "3 columns found with 11 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'at', '.l', 'rh'])\n",
      "3 columns found with 14 missing values (column name overlap: ['smri_vol_cort.destrieux_', 'al.', 's.', 'an'])\n",
      "3 columns found with 22 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "3 columns found with 27 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'ol'])\n",
      "3 columns found with 29 missing values (column name overlap: ['_cort.destrieux_', 'smri_', '.su', '.rh'])\n",
      "3 columns found with 624 missing values (column name overlap: ['_cort.destrieux_mean', 'smri_t2w.'])\n",
      "3 columns found with 620 missing values (column name overlap: ['_cort.destrieux_mean.lh', 'smri_t2w.'])\n",
      "3 columns found with 619 missing values (column name overlap: ['_cort.destrieux_s.interm.prim.jensen.lh', 'smri_t2w.'])\n",
      "3 columns found with 618 missing values (column name overlap: ['_cort.destrieux_s.temporal.transverse.lh', 'smri_t2w.'])\n",
      "3 columns found with 38 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s'])\n",
      "3 columns found with 42 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', 's.'])\n",
      "3 columns found with 46 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s', 'd.'])\n",
      "3 columns found with 45 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "2 columns found with 36 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'ral.', 'po', 'rh', 'in'])\n",
      "2 columns found with 19 missing values (column name overlap: ['_cort.destrieux_', 'ital.', 'smri_', 'nt.', '.o'])\n",
      "2 columns found with 49 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', '.p'])\n",
      "2 columns found with 13 missing values (column name overlap: ['_cort.destrieux_', 'smri_', '.rh', 'ar', 'la'])\n",
      "2 columns found with 15 missing values (column name overlap: ['smri_area_cort.destrieux_', 's.', '.r', 'ct'])\n",
      "2 columns found with 34 missing values (column name overlap: ['_cort.destrieux_', 'ral.rh', 'smri_', 's.'])\n",
      "2 columns found with 18 missing values (column name overlap: ['smri_area_cort.destrieux_', 'le.'])\n",
      "2 columns found with 30 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'at', 'l.', 'po', 'ra'])\n",
      "2 columns found with 20 missing values (column name overlap: ['smri_area_cort.destrieux_g.and.s.', 'ci', 'l.'])\n",
      "2 columns found with 33 missing values (column name overlap: ['smri_vol_cort.destrieux_g.', 'tra', 't.', 'sv'])\n",
      "2 columns found with 24 missing values (column name overlap: ['smri_vol_cort.destrieux_'])\n",
      "2 columns found with 25 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'al.'])\n",
      "2 columns found with 23 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', 't.lh', '.in', 'ar'])\n",
      "\n",
      "Loaded Shape: (11534, 1510)\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Data(loc = [data1, data2],\n",
    "             unique_val_warn = None,\n",
    "             drop_na = False,\n",
    "             clear_existing = True # Make sure to call clear_existing if re-loading the same data!\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are provided some more detailed information about what NaN columns are loaded. This is useful because we can note there are 435 columns found with 617 missing values, and of those 435 columns, 'smri_t2w.' is one of the keys present in all of them. In this case, what we can do is try not loading t2w rois, as it is clear some number of subjects just didn't get a t2 scan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared loaded data.\n",
      "\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp101.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp101_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp201.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp201_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "\n",
      "Dropped 453 columns per passed drop_keys argument\n",
      "Dropped 0 cols for all missing values\n",
      "Loaded rows with NaN remaining: 2396\n",
      "Loaded NaN Info:\n",
      "There are: 4680 total missing values\n",
      "437 columns found with 4 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "18 columns found with 1 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "12 columns found with 5 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "11 columns found with 8 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "10 columns found with 2 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "10 columns found with 3 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "7 columns found with 9 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "6 columns found with 12 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "5 columns found with 43 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "5 columns found with 35 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "4 columns found with 39 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 's.'])\n",
      "3 columns found with 7 missing values (column name overlap: ['_cort.destrieux_mean.lh', 'smri_t1w.'])\n",
      "3 columns found with 11 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'at', '.l', 'rh'])\n",
      "3 columns found with 22 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "3 columns found with 27 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'ol'])\n",
      "3 columns found with 29 missing values (column name overlap: ['_cort.destrieux_', 'smri_', '.su', '.rh'])\n",
      "3 columns found with 14 missing values (column name overlap: ['smri_vol_cort.destrieux_', 'al.', 's.', 'an'])\n",
      "3 columns found with 6 missing values (column name overlap: ['_cort.destrieux_s.interm.prim.jensen.lh', 'smri_t1w.'])\n",
      "3 columns found with 38 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s'])\n",
      "3 columns found with 42 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', 's.'])\n",
      "3 columns found with 46 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s', 'd.'])\n",
      "3 columns found with 45 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "2 columns found with 13 missing values (column name overlap: ['_cort.destrieux_', 'smri_', '.rh', 'ar', 'la'])\n",
      "2 columns found with 20 missing values (column name overlap: ['smri_area_cort.destrieux_g.and.s.', 'ci', 'l.'])\n",
      "2 columns found with 15 missing values (column name overlap: ['smri_area_cort.destrieux_', 's.', '.r', 'ct'])\n",
      "2 columns found with 49 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', '.p'])\n",
      "2 columns found with 18 missing values (column name overlap: ['smri_area_cort.destrieux_', 'le.'])\n",
      "2 columns found with 19 missing values (column name overlap: ['_cort.destrieux_', 'ital.', 'smri_', 'nt.', '.o'])\n",
      "2 columns found with 23 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', 't.lh', '.in', 'ar'])\n",
      "2 columns found with 34 missing values (column name overlap: ['_cort.destrieux_', 'ral.rh', 'smri_', 's.'])\n",
      "2 columns found with 24 missing values (column name overlap: ['smri_vol_cort.destrieux_'])\n",
      "2 columns found with 25 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'al.'])\n",
      "2 columns found with 30 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'at', 'l.', 'po', 'ra'])\n",
      "2 columns found with 36 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'ral.', 'po', 'rh', 'in'])\n",
      "2 columns found with 33 missing values (column name overlap: ['smri_vol_cort.destrieux_g.', 'tra', 't.', 'sv'])\n",
      "\n",
      "Loaded Shape: (11534, 1057)\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Data(loc = [data1, data2],\n",
    "             unique_val_warn = None,\n",
    "             drop_keys = 'smri_t2w.',\n",
    "             drop_na = False,\n",
    "             clear_existing = True\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a few different options for how we want to handle the rest of the NaN's. The first is to either just drop all of them, but as evident by 437 columns found with 4 missing values, it might be the case that there are just a few subjects with a messed up scan, and therefore a bunch of NaN's. We can try setting drop_nan to 10. In that case, only subjects with less then or equal to 10 NaNs will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared loaded data.\n",
      "\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp101.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp101_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp201.txt  with dataset type: basic\n",
      "dropped ['collection_id', 'abcd_mrisdp201_id', 'dataset_id', 'subjectkey', 'interview_age', 'interview_date', 'sex', 'collection_title', 'study_cohort_name'] columns by default  due to dataset type\n",
      "\n",
      "Dropped 453 columns per passed drop_keys argument\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 12 rows for missing values, based on the provided drop_na param: 10 with actual na_thresh: 10\n",
      "Loaded rows with NaN remaining: 2384\n",
      "Loaded NaN Info:\n",
      "There are: 2697 total missing values\n",
      "12 columns found with 1 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "7 columns found with 2 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "6 columns found with 3 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "5 columns found with 35 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "5 columns found with 43 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "5 columns found with 22 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "4 columns found with 9 missing values (column name overlap: ['_cort.destrieux_', 'smri_'])\n",
      "4 columns found with 45 missing values (column name overlap: ['smri_area_cort.destrieux_'])\n",
      "4 columns found with 39 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 's.'])\n",
      "3 columns found with 8 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 't.', 's.'])\n",
      "3 columns found with 11 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'at', '.l', 'rh'])\n",
      "3 columns found with 12 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 't.lh', 'ol', 's.'])\n",
      "3 columns found with 14 missing values (column name overlap: ['smri_vol_cort.destrieux_', 'al.', 's.', 'an'])\n",
      "3 columns found with 15 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 's.'])\n",
      "3 columns found with 25 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'al.'])\n",
      "3 columns found with 27 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'ol'])\n",
      "3 columns found with 42 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', 's.'])\n",
      "2 columns found with 34 missing values (column name overlap: ['_cort.destrieux_', 'ral.rh', 'smri_', 's.'])\n",
      "2 columns found with 20 missing values (column name overlap: ['smri_area_cort.destrieux_g.and.s.', 'ci', 'l.'])\n",
      "2 columns found with 4 missing values (column name overlap: ['smri_vol_cort.destrieux_', '.rh', 'd.', 's.'])\n",
      "2 columns found with 5 missing values (column name overlap: ['smri_area_cort.destrieux_', 's.'])\n",
      "2 columns found with 49 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', '.p'])\n",
      "2 columns found with 46 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s', 'd.'])\n",
      "2 columns found with 13 missing values (column name overlap: ['_cort.destrieux_', 'smri_', '.rh', 'ar', 'la'])\n",
      "2 columns found with 38 missing values (column name overlap: ['smri_area_cort.destrieux_', '.s'])\n",
      "2 columns found with 33 missing values (column name overlap: ['smri_vol_cort.destrieux_g.', 'tra', 't.', 'sv'])\n",
      "2 columns found with 19 missing values (column name overlap: ['_cort.destrieux_', 'ital.', 'smri_', 'nt.', '.o'])\n",
      "2 columns found with 18 missing values (column name overlap: ['smri_area_cort.destrieux_', 'le.'])\n",
      "2 columns found with 37 missing values (column name overlap: ['smri_area_cort.destrieux_', '.lh', 'ar', 's.', 'in', '.f'])\n",
      "2 columns found with 36 missing values (column name overlap: ['_cort.destrieux_', 'smri_', 'ral.', 'po', 'rh', 'in'])\n",
      "2 columns found with 29 missing values (column name overlap: ['smri_area_cort.destrieux_', '.sup.p', '.rh', 'la'])\n",
      "2 columns found with 30 missing values (column name overlap: ['_cort.destrieux_s.', 'smri_', '.lh', 'at', 'l.', 'po', 'ra'])\n",
      "\n",
      "Loaded Shape: (11522, 1057)\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Data(loc = [data1, data2],\n",
    "             unique_val_warn = None,\n",
    "             drop_keys = 'smri_t2w.',\n",
    "             drop_na = 10,\n",
    "             clear_existing = True\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great. Now we have 2384 subjects with atleast one NaN value to impute, but we know that all of those subjects only have 1-10 missing values, and likewise, no single feature seems to have all that many NaN's. We now have 11522 subjects and still 1057 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data distribution visually too for a few random ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this function will break unless you have ffmpeg installed on your machine, so it is commented out here.\n",
    "# It makes a gif of your loaded data distribution\n",
    "#ML.Show_Data_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not appear to be too many big outliers in the dataset (some, but...), lets avoid doing any outlier dropping in this example, and instead if it is a problem deal with it during modelling. In general, there are a number of methods for dropping outliers or performing other proc on the loaded data either built into Load_Data or as an external function, e.g., Proc_Data_Unique_Cols.\n",
    "\n",
    "We will load our target variable next. See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#load-targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a number of the default load params are accesible when loading targets too in case they need to change. We again need to only use fairly simple functionality here, loading in the sex column from our structural mris (as some bugs with sex were fixed in release 2.0.1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/sage/work/ABCDFixRelease2p0p1/abcd_mrisdp101.txt  with dataset type: basic\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 1 rows for missing values, based on the provided drop_na param: True with actual na_thresh: 0\n",
      "Loaded rows with NaN remaining: 0\n",
      "\n",
      "loading: sex\n",
      "10\n",
      "\n",
      "Loaded Shape: (11533, 1)\n",
      "All loaded targets\n",
      "0 : sex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Targets(loc = data1,\n",
    "                col_name = 'sex',\n",
    "                data_type = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set to overlapping loaded subjects.\n",
      "-- sex --\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Name</th>\n",
       "      <th>Counts</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internal_Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>5483</td>\n",
       "      <td>0.475914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>6038</td>\n",
       "      <td>0.524086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Original_Name  Counts  Frequency\n",
       "Internal_Name                                 \n",
       "0                         F    5483   0.475914\n",
       "1                         M    6038   0.524086"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Show_Targets_Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note when plotting the Show_Targets_Dist is the printed \"Set to overlapping loaded subjects.\", which means we are only seeing the targets distribution based on the overlap of all subjects currently loaded (so right now that is just the overlap with data). This can be set off in the case we want to see, in this case, the few subjects who will be eventually lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look into adding covars next. Where co-variates arn't quite treated as typical co-variates, but are values we would like to be able to pass as additional input to the ML model if desired (and input that can be treated with special care). This example will actually skip this step though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will considering loading different stratification values. These are the values that we can optionally define custom validation / split behavior on. Within this example, we are just going to make sure that all splits preserve subjects with the same family id within the same fold, so lets load family id - after looking as the help function. See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#load-strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/sage/work/ABCD2p0NDA/acspsw03.txt  with dataset type: basic\n",
      "Dropped 0 cols for all missing values\n",
      "Dropped 2 rows for missing values, based on the provided drop_na param: True with actual na_thresh: 0\n",
      "Loaded rows with NaN remaining: 0\n",
      "Loaded Shape: (11873, 1)\n"
     ]
    }
   ],
   "source": [
    "ML.Load_Strat(loc=strat1,\n",
    "              col_name='rel_family_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have all the data we will need loaded (Noting that the minimum requiriments for running an ML expiriment are just data or covars and targets, the rest being optional). The actual length of the script is also not as terrible as it seems, and once loading behavior is confirmed, verbose can even be turned off. In practice also, there is no reason why the user should not just keep reloading data, with changes to params, within the same cell by re-running it - which would defeat the point of a tutorial, but would greatly help readability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move onto defining our validation stratagy (which is again optional, but as stated before for this example we are going to preserve family ids within the same folds), if no explicit validation strategy is defined, then random splits will be used. See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#validation-phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for group preserving behavior, as we are interested in keep families within the same folds, we supply an argument for groups. Specifically, we use the name of the column loaded within self.strat\n",
    "\n",
    "Notably as well is that defining a validation strategy is used to define global split behavior, which means the defined validation behavior will effect the train test split, as well as the internal splits used in modelling. We can re-define the original validation strategy at any point, even after defining a global train test split (why? e.g., in order to test within just the train set the effect of putting all the subjects with any nan as train only)\n",
    "\n",
    "The extra feature we will add to the split is defining all subjects with any missing measurements to be train only, this means these subjects will always be put in the training fold, in both the global split and for internal CV and even nested internal parameter search CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling Prepare_All_Data() to change the default merge behavior call it again!\n",
      "Preparing final data, in self.all_data\n",
      "Any changes to loaded data, covars or strat will not be included, from now on.\n",
      "\n",
      "Final data (w/ target) for modeling loaded shape: (11519, 1059)\n",
      "2384 Train only subjects defined.\n",
      "Those subjects are excluded from the below stats!\n",
      "\n",
      "CV defined with group preserving over 8029 unique values.\n"
     ]
    }
   ],
   "source": [
    "ML.Define_Validation_Strategy(groups='rel_family_id', train_only_subjects='nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note explictly, \"Calling Prepare_All_Data() to change the default merge behavior call it again!\n",
    "Preparing final data, in self.all_data\".\n",
    "\n",
    "If you want to load new data or a new target for example, or change how the data is merged together (i.e., NaN behavior), you can manually re-call this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly before we get to modelling, we want to define a global train-test split, so that we can perform model exploration, and parameter tuning ect... on a training set, and leave a left-out testing set to eventually test with out final selected model. See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing split on 8516 subjects with 3003 considered train only!\n",
      "random_state: 1\n",
      "Test split size: 0.35\n",
      "\n",
      "Performed train test split\n",
      "Train size: 8549\n",
      "Test size:  2970\n"
     ]
    }
   ],
   "source": [
    "ML.Train_Test_Split(test_size=.35) #Lets use .35, as train only subjects will not be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above for the curious, 3003 subjects are considered train only, event though we only had 2384 with NaN. This is because we are defining group preserving behavior on family id, so not only do we need to keep those subjects in the training set, but keep all of their family members in the training set. We know now that no family id is in both the train and test set - but for the perhaps rightfully paranoid we can make sure of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique family ids in train:  7123\n",
      "Unique family ids in test:  2600\n",
      "Overlap :  0\n"
     ]
    }
   ],
   "source": [
    "train_ids = set(ML.strat['rel_family_id_Strat'].loc[ML.train_subjects])\n",
    "test_ids = set(ML.strat['rel_family_id_Strat'].loc[ML.test_subjects])\n",
    "\n",
    "print('Unique family ids in train: ', len(train_ids))\n",
    "print('Unique family ids in test: ', len(test_ids))\n",
    "print('Overlap : ', len(train_ids.intersection(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here might also be a good time to save a copy of the object we have created, if that is desired behavior. We are going to save it in our defined log_dr, and use the low_memory flag, which doesn't save explicitly loaded self.data, self.covars ect... and just stores the final self.all_data for modelling (which we have already implicitly created). Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = os.path.join(ML.exp_log_dr, 'Sex.ML')\n",
    "ML.Save(save_loc, low_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now first thing to note is, since we set low_memory to false, we will can see that data, and targets ect.. are now deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0), (0, 0))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML.data.shape, ML.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And only all_data remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11519, 1059)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML.all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example now delete our original ML object and then re-load the saved one. Note we need to import the Load function from BPt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML object loaded from save!\n"
     ]
    }
   ],
   "source": [
    "from BPt import Load\n",
    "\n",
    "del ML\n",
    "ML = Load(save_loc, existing_log='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11519, 1059)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML.all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within modeling, what we are concerned with is building and first Evaluating different Model_Pipelines. We will first start though by defing what is called a Problem_Spec, which contains essentially our specifications for what ML expiriment we want to run exactly. This is imported from the library as well. We will be doing a lot of imports in this next section, in particular to access the modelling objects described at https://bpt.readthedocs.io/en/latest/param_objects.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt import Problem_Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Problem_Spec(n_jobs=8, problem_type='binary', scorer=['roc_auc'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_spec = Problem_Spec(problem_type = 'binary', # As this is a binary problem\n",
    "                            scorer = ['roc_auc'], # Good thresholded and non-thresholded binary metrics\n",
    "                            n_jobs = 8)\n",
    "problem_spec               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify some default parameters for different levels of verbose output during modelling. See: https://bpt.readthedocs.io/en/latest/BPt_class_docs.html#set-default-ml-verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default ML verbosity set within self.default_ML_verbosity.\n",
      "----------------------\n",
      "save_results: False\n",
      "progress_bar: True\n",
      "progress_loc: None\n",
      "compute_train_score: False\n",
      "show_init_params: True\n",
      "fold_name: False\n",
      "time_per_fold: False\n",
      "score_per_fold: False\n",
      "fold_sizes: False\n",
      "best_params: False\n",
      "save_to_logs: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML.Set_Default_ML_Verbosity()  # Default are fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to building our Model_Pipeline - where a Model_Pipeline is constructed as a number of individual Pipeline pieces, all of which we have imported. Let's start by first just specifying a Model_Pipeline with just the default pre-set values for everything but Model, we will specify a custom Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this next cell we need the Model object and the Model_Pipeline object\n",
    "from BPt import Model, Model_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Pipeline\n",
      "--------------\n",
      "imputers=\\\n",
      "[Imputer(extra_params={}, obj='mean', scope='float'),\n",
      " Imputer(extra_params={}, obj='median', scope='cat')]\n",
      "\n",
      "scalers=\\\n",
      "Scaler(extra_params={}, obj='standard')\n",
      "\n",
      "model=\\\n",
      "Model(extra_params={}, obj='dt')\n",
      "\n",
      "param_search=\\\n",
      "None\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will use a decision tree as our base model\n",
    "model = Model('dt')\n",
    "\n",
    "model_pipeline = Model_Pipeline(model = model)\n",
    "model_pipeline.print_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the their are a few default values, specifically we have a set of default imputers, one for replacing all float variables with the mean value, and one for replacing all categorical / binary variables (if any, otherwise ignored) with the median values.\n",
    "\n",
    "Next, we have a just standard scaler, which scales all features to have mean 0, std of 1.\n",
    "\n",
    "Then, we have our decision tree.\n",
    "\n",
    "Lastly, we have no param_search specified, and also we are keeping track of 'base' feature importances, which means for a linear model, just the beta weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an initial model, we are ready to use the Evaluate function, see: https://abcd-ml.readthedocs.io/en/latest/ABCD_ML_class_docs.html#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Pipeline\n",
      "--------------\n",
      "imputers=\\\n",
      "[Imputer(extra_params={}, obj='mean', scope='float'),\n",
      " Imputer(extra_params={}, obj='median', scope='cat')]\n",
      "\n",
      "scalers=\\\n",
      "Scaler(extra_params={}, obj='standard')\n",
      "\n",
      "model=\\\n",
      "Model(extra_params={}, obj='dt')\n",
      "\n",
      "param_search=\\\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Problem_Spec\n",
      "------------\n",
      "problem_type = binary\n",
      "target = sex\n",
      "scorer = ['roc_auc']\n",
      "weight_scorer = False\n",
      "scope = all\n",
      "subjects = all\n",
      "len(subjects) = 11519 (before overlap w/ train/test subjects)\n",
      "n_jobs 8\n",
      "random_state 1\n",
      "\n",
      "Evaluate Params\n",
      "---------------\n",
      "splits = 3\n",
      "n_repeats = 1\n",
      "CV = default\n",
      "train_subjects = train\n",
      "feat_importances = Feat_Importance(obj='base', shap_params=None)\n",
      "len(train_subjects) = 8549 (before overlap w/ problem_spec.subjects)\n",
      "run_name = dt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6a5da6e8704522bb55b3c1ae68d544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repeats', max=1.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f78576d506a40e391fa19a8886af01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Folds', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-29-389ad565c17d>\", line 4, in <module>\n",
      "    n_repeats = 1)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/BPt/main/_ML.py\", line 520, in Evaluate\n",
      "    splits, n_repeats, splits_vals)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/BPt/pipeline/Evaluator.py\", line 156, in Evaluate\n",
      "    test_subjects, fold_ind)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/BPt/pipeline/Evaluator.py\", line 303, in Test\n",
      "    self._train_model(train_data)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/BPt/pipeline/Evaluator.py\", line 529, in _train_model\n",
      "    self.model.fit(X, y, train_data_index=train_data.index)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/BPt/pipeline/BPt_Pipeline.py\", line 49, in fit\n",
      "    super().fit(X, y, **fit_params)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/sklearn/pipeline.py\", line 335, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/sklearn/tree/_classes.py\", line 894, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/sklearn/tree/_classes.py\", line 375, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/sage/anaconda3/envs/home/lib/python3.7/posixpath.py\", line 414, in _joinrealpath\n",
      "    while rest:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      splits = 3,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the Evaluate call are returned as dictionary of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, its clear that running the full evaluation even with a very quick simple model like the decision tree if very intensive, all we have 1000+ features and thousands of subjects. One optional trick we can use, especially for this example, is to run expiriments with only a subset of the features, let's consider what we have loaded:\n",
    "\n",
    "- 'smri_thick_cort'\n",
    "- 'smri_sulc_cort'\n",
    "- 'smri_area_cort'\n",
    "- 'smri_vol_cort'\n",
    "- 'smri_t1w.white02_cort'\n",
    "- 'smri_t1w.gray02_cort'\n",
    "- 'smri_t1w.contrast_cort'\n",
    "\n",
    "Alright, so let's say we are only interested in cortical thickness - to limit our Evaluate call to using just cortical thickness features, all we have to do is provide that substring from above, 'smri_thick_cort', to the scope of our problem_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Pipeline\n",
      "--------------\n",
      "imputers=\\\n",
      "[Imputer(extra_params={}, obj='mean', scope='float'),\n",
      " Imputer(extra_params={}, obj='median', scope='cat')]\n",
      "\n",
      "scalers=\\\n",
      "Scaler(extra_params={}, obj='standard')\n",
      "\n",
      "model=\\\n",
      "Model(extra_params={}, obj='dt')\n",
      "\n",
      "param_search=\\\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Problem_Spec\n",
      "------------\n",
      "problem_type = binary\n",
      "target = sex\n",
      "scorer = ['roc_auc']\n",
      "weight_scorer = False\n",
      "scope = smri_thick_cort\n",
      "subjects = all\n",
      "len(subjects) = 11519 (before overlap w/ train/test subjects)\n",
      "n_jobs 8\n",
      "random_state 1\n",
      "\n",
      "Evaluate Params\n",
      "---------------\n",
      "splits = 3\n",
      "n_repeats = 1\n",
      "CV = default\n",
      "train_subjects = train\n",
      "feat_importances = Feat_Importance(obj='base', shap_params=None)\n",
      "len(train_subjects) = 8549 (before overlap w/ problem_spec.subjects)\n",
      "run_name = dt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0a80d0c4bc423bb7b106651d93390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repeats', max=1.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469de9f49d634e0d81a8e7e4274a51d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Folds', max=3.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Validation Scores\n",
      "_________________\n",
      "Scorer:  roc_auc\n",
      "Mean Validation score:  0.5500422740829001\n",
      "Std in Validation score:  0.0022197062931817533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instead of defining a new problem_spec object, we can just update the value\n",
    "problem_spec.scope = 'smri_thick_cort'\n",
    "\n",
    "# Now re-run Evaluate\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      splits = 3,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this model did worse, we really didn't expect much from the decision tree model, and it was faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a just linear model now (Logistic Regression, since binary), updating our model within our model_pipeline first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.model = Model('linear')\n",
    "\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A just linear model appears to do much better then the decision tree. Before we try other models, let's take a look at the beta weights from the linear model (averaged over the 6 models, 2 repeated, 3 folds). A short hand for this is by just calling plot global feat importances directly, which will plot the feature importances for the last Evaluate call, though it is also worth exploring the results dictionary which is returned from Evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Global_Feat_Importances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we passed multiple feature importances to calculate they would appear in a list, \n",
    "print(len(results['FIs']))\n",
    "\n",
    "# If we show this object we see\n",
    "results['FIs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either look at the importances in a dataframe, or pass the feature importance object to the plotting func\n",
    "results['FIs'][0].global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML.Plot_Global_Feat_Importances(results['FIs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so the biggest features driving the model to predict female (0) vs. male (1) are the mean thickness of the different hemispheres. Luckily, we are not interested in actually solving a problem in this example, but instead illustrating usage of the library, so lets move on to trying some more advanced models.\n",
    "\n",
    "Let's explore some different models before sticking with this straight linear one. One of my favorites to try is light gradient boosting machine, or light gbm (or lgbm). \n",
    "\n",
    "Note: lightgbm is not included in ABCD_ML's pip requiriments by default as on mac's especially there are some extra download steps. If you have not already, and would like to use the lightgbm package, please install the lightgbm packagae on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.model = Model('light gbm')\n",
    "\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point we have not been exploring any nested hyper-parameters, which for a linear and decision tree model is not a big deal, but fine tuning the lightgbm is fairly essential in order to get good performance. To tune parameters facebooks nevergrad package is used a backend optimizer which allows for a lot of interesting search types (https://bpt.readthedocs.io/en/latest/search_types.html), but introduces a little bit of a learning curve for those interested in entering in custom hyperparameter distributions to search over.\n",
    "\n",
    "By default though, most classifiers have atleast one pre-set hyperparameter distributions to search over. We can see lightgbm's at https://bpt.readthedocs.io/en/latest/options.html#light-gbm-classifier. Our choice here of what search type to use will be passed to the search_type param, and we will just start with a RandomSearch.\n",
    "\n",
    "To specify that we want to use a parameter search with the light gbm we have to do two things: First specify the param distribution we want to use in the model object itself, second, we want to create a param_search object within out model_pipeline, where right now it is set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt import Param_Search\n",
    "\n",
    "model = Model(obj = 'light gbm',\n",
    "              params = 1 # To specify we want to use the preset param dist w/ index 1\n",
    "             )\n",
    "\n",
    "param_search = Param_Search(search_type = 'RandomSearch',\n",
    "                            n_iter = 50)\n",
    "\n",
    "# Now we can update our model_pipeline\n",
    "model_pipeline.model = model\n",
    "model_pipeline.param_search = param_search\n",
    "\n",
    "# And re-run Evaluate\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of a search over params, our linear model still performs best.\n",
    "Let's try some other linear models then-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.model = Model('elastic net', params=1)\n",
    "\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.model = Model('ridge', params=1)\n",
    "\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are besides model, a whole bunch of other pipeline pieces you can change, e.g., scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPt import Scaler\n",
    "model_pipeline.scaler = Scaler('robust')\n",
    "\n",
    "results = ML.Evaluate(model_pipeline = model_pipeline,\n",
    "                      problem_spec = problem_spec,\n",
    "                      n_repeats = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it for this example - for now, I have plans though to update it more with a more detailed dive into modeling --- and also to include the Test feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('home': conda)",
   "language": "python",
   "name": "python37664bithomeconda2aade2e1d0ce4797afe91f4891a59d68"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
